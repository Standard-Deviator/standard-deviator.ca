---
title: 'Part 1 of 3: The Rock'
author: Mark Christopher Adkins
date: '2020-03-09'
slug: part-1-the-rock
categories:
  - Blog Post
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2020-03-09T22:18:25-04:00'
featured: no
draft: true
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div style="position: relative;">
<p><img src="https://media.giphy.com/media/xT0BKumCMrUb0dCypa/200w_d.gif" /></p>
</div>
<!-- <img style="float: right;" src="https://media.giphy.com/media/xT0BKumCMrUb0dCypa/200w_d.gif"> -->
<p>There is a strange feeling of discomfort growing in me, yet I’m not sure when it started.</p>
<p>I’m in the midst of my formal training as a quantitative methodologist in the social sciences, yet I feel out of place. Up until this point my training has predominantly focused on the procedural application of Neyman-Pearson null hypothesis statistical testing (NHST), but I’m no longer convinced this framework (as it is used within psychology) answers the types of questions which social scientists are enquiring about. As scientists, we are taught to setup the lofty target of uncovering truth about either the world around or within us, but we are ill-equipped to critically assess the quality of evidence we observe. Even worse, the type of confirming evidence we often seek precludes any chance of falsifying any hypothesis we are exploring. It really makes me take a step back and ask, what are we even doing?</p>
<p>I feel like my training is what permits me to step back and pose such a question, while simultaneously instilling a sense of responsibility within me. This sense of responsibility is particularly salient when I’m approached by statistical consulting clients and fellow graduate students seeking either statistical or methodological advice. They assume that I can tell them the “proper” way to analyze their data. There is an almost palpable and ever-present desire for concrete answers which dictate specific analyses with no room for ambiguity. Figure 1 shows an example of a flow chart I found in a published article (which students are taught to consult in order to identify the appropriate analysis. The fact of the matter is that research is a dynamic process filled with decisions, each of which requires rational justification. EACH of these decisions plays a vital role in how we interpret the outcome of our research. Analyses themselves simply cannot magically provide answers, and the answers they do provide are, by their very nature, always probabilistic and are shaped by how we pose our research question. Sadly, most clients’ exposure to statistics and methodology is limited, and the dynamic aspect of research is lost along the way. This is when my sense of responsibility kicks in and I start to cram in mini statistics lectures alongside my consulting advice. Ultimately, my aim is to do my best to ensure that researchers understand why I suggest certain paths of actions. I want to walk them through the dynamic process and highlight the importance of each step along the journey.</p>
<p>I suspect we lose this dynamic aspect of research for two reasons: the first is practical, while the second is not-so-much. In practical terms, it’s hard to document every decision made during the process of planning out and conducting research. Aside from using version-control software with built in documentation of changes (such as Git), it can be quite difficult to keep track of each step along the decision tree which we are traversing.
The later reason needs a bit more elaboration. So, let’s get into it.</p>
<p>The published article. The holy grail of the graduate student’s academic life. Many published articles I have read to date paint a compelling picture that through rigor and foresight, researchers are uncovering truths about interesting phenomena. The picture that is painted is often of a logically sequential process. It began at point A with an interesting question/hypothesis and concludes at point D with an interpretation of results. Each subsequent point is a logical progression along this path of discovery. This misrepresents the dynamic nature of the process itself. At each point researchers are faced with decisions, and some of these decisions can even reshape our original research questions. Even the most well-planned line of research cannot foresee every problem. Consequently, research results could be very be different had we made different decision at any point along the road. At the end of the line, we turn around to view the path behind us, and we forget (or fail to note) that we proceeded down some “dead-end” paths before back-tracking to find another path. The process is either not documented and lost, or it falls by the wayside as unimportant information which is lost in the editorial process. We delude ourselves into thinking that we followed the TRUE path right from the beginning, and we were rewarded for our efforts by discovering evidence in support of our phenomena of interest.</p>
<p>This is perhaps the most optimistic view I could have of the publishing/research process. However, it is rapidly becoming untenable to me because there is mounting evidence that some unknown, and substantially non-zero, proportion of social science researchers are engaging (either knowingly or unknowingly) in questionable research practices (QRPs) while searching for “truth”.
On top of QRPs, we seem to have a backward approach about how we conduct research. Our training as students thus far follows the flow-chart type style taught to undergraduate statistics. We try to fit our research questions into statistical models/analyses which we know about. This both restricts the questions we want to explore and frequently provides answers to questions we are not even asking. Try to tell me with a straight face that this approach to statistics makes sense to you. I often receive questions about how to conduct an ANOVA or multiple regression (never mind that these are both special cases of the general linear model) or other such tests, even if the results provided by these analyses are logically disconnected from their proposed research questions. There seems to be a fundamental mismatch between how we understand methodology broadly and how we conduct our research in practice. But this is how students are taught to think about statistics.</p>
<p>At last I arrive at the source of my discontent. Right from our initial statistical training, we are doomed to fail. It should be no surprise that like ancient Sisyphus, we seem to be training the next generation of researchers to push the boulder back up the hill again only to have it roll back down again.</p>
